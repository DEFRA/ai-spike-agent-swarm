{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c79849",
   "metadata": {},
   "source": [
    "# Managed Agent Swarm - Policy Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explores a **managed agent swarm** pattern where a central orchestrator coordinates multiple specialist AI agents to collaboratively analyze farming policy documents. Unlike autonomous swarms where agents self-organize, or round-robin approaches where agents take predetermined turns, the managed swarm uses an intelligent orchestrator to dynamically direct the conversation based on the emerging discussion.\n",
    "\n",
    "### The Experiment\n",
    "\n",
    "The core idea being tested is whether an LLM-powered orchestrator can effectively facilitate a productive multi-agent dialogue by:\n",
    "- Deciding which specialist to engage at each step\n",
    "- Creating connections between agents' observations\n",
    "- Directing follow-up questions that build on prior findings\n",
    "- Knowing when sufficient insights have been gathered to synthesize a final report\n",
    "\n",
    "### Use Case: Policy Document Analysis\n",
    "\n",
    "We apply this pattern to analyze UK farming policy documents across three dimensions:\n",
    "- **Critique Agent**: Analyzes how the document is written (clarity, structure, tone, accessibility)\n",
    "- **Gap Analysis Agent**: Identifies missing information and coverage gaps\n",
    "- **Ambiguity Agent**: Detects unclear language and vague requirements\n",
    "\n",
    "The orchestrator facilitates a group discussion between these specialists, encouraging them to respond to each other's findings rather than working in isolation. This collaborative approach aims to surface insights that might be missed by sequential or independent analysis.\n",
    "\n",
    "### Key Mechanisms\n",
    "\n",
    "**Shared Message History**: All agents see the full conversation, enabling them to reference and build on each other's observations.\n",
    "\n",
    "**Dynamic Tool Selection**: The orchestrator chooses which specialist to engage based on the current state of the discussion, not a fixed sequence.\n",
    "\n",
    "**Human-in-the-Loop Handoff**: When the orchestrator determines sufficient analysis has been completed, it hands off to a report writer agent. A human approval step allows for feedback or acceptance before final report generation.\n",
    "\n",
    "**Usage Limits**: Token and tool call limits prevent runaway execution while encouraging focused, efficient analysis.\n",
    "\n",
    "---\n",
    "\n",
    "For environment setup and prerequisites, see the [README.md](../../README.md) in the repository root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20113f76",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Configuration for the notebook environment. We use `python-dotenv` to load environment variables from `../notebooks/.env`, which contains AWS credentials for Bedrock access and optional OpenTelemetry configuration.\n",
    "\n",
    "See the [README.md](../../README.md) for details on required environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf18304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic_ai\n",
    "from pydantic_ai.models import bedrock as bedrock_models\n",
    "\n",
    "claude_haiku = bedrock_models.BedrockConverseModel(\n",
    "    model_name=\"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    ")\n",
    "\n",
    "claude_sonnet = bedrock_models.BedrockConverseModel(\n",
    "    model_name=\"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    ")\n",
    "\n",
    "bedrock_model_settings = bedrock_models.BedrockModelSettings(\n",
    "    bedrock_guardrail_config={\"trace\": \"enabled\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3516f7",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "We configure two Claude models from Amazon Bedrock via the `pydantic-ai` library:\n",
    "\n",
    "- **Claude Haiku**: Fast, lightweight model used for the orchestrator and specialist agents. Ideal for quick analysis and coordination tasks.\n",
    "- **Claude Sonnet**: More powerful model reserved for the report writer agent, which needs to synthesize complex information into a cohesive final report.\n",
    "\n",
    "The `BedrockModelSettings` is shown here to demonstrate that Bedrock clients can be customised as needed for the application of guardrails, logging, or other settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19327fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "from pydantic_ai.messages import (\n",
    "    ModelMessage,\n",
    "    ModelRequest,\n",
    "    ModelResponse,\n",
    "    TextPart,\n",
    "    UserPromptPart,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentDependencies:\n",
    "    source_policy: str\n",
    "    messages: list[ModelMessage] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf5e7d",
   "metadata": {},
   "source": [
    "## Agent Dependencies\n",
    "\n",
    "The `AgentDependencies` dataclass defines shared state that flows through all agent interactions:\n",
    "\n",
    "- **source_policy**: The farming policy document text being analyzed\n",
    "- **messages**: Shared conversation history that enables agents to see and reference prior observations\n",
    "\n",
    "Pydantic AI recommends using a dependency injection pattern to manage shared state across agents,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c968a",
   "metadata": {},
   "source": [
    "## OpenTelemetry Tracing Setup\n",
    "\n",
    "Optional observability configuration using OpenTelemetry to trace agent interactions. When `OTEL_EXPORTER_OTLP_ENDPOINT` is set, this sends detailed execution traces (spans) to an OTLP collector.\n",
    "\n",
    "Pydantic AI has built-in support for OpenTelemetry tracing of agent operations. This could be useful if we get asked to demonstrate or debug the multi-agent interactions in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.trace import set_tracer_provider\n",
    "\n",
    "\n",
    "def setup_tracing():\n",
    "    if not os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\"):\n",
    "        print(\"OTEL_EXPORTER_OTLP_ENDPOINT not set, skipping tracing setup\")\n",
    "        return\n",
    "\n",
    "    os.environ[\"OTEL_SERVICE_NAME\"] = \"managed-agent-swarm\"\n",
    "\n",
    "    exporter = OTLPSpanExporter()\n",
    "    span_processor = BatchSpanProcessor(exporter)\n",
    "    tracer_provider = TracerProvider()\n",
    "    tracer_provider.add_span_processor(span_processor)\n",
    "\n",
    "    set_tracer_provider(tracer_provider)\n",
    "    return\n",
    "\n",
    "\n",
    "setup_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f74d9e",
   "metadata": {},
   "source": [
    "## Swarm Architecture Diagram\n",
    "\n",
    "This Mermaid flowchart visualizes the managed swarm architecture:\n",
    "\n",
    "- **Orchestrator**: The central coordinator that maintains conversation history and decides which specialist to engage\n",
    "- **Three Specialist Agents**: Critique, Gap Analysis, and Ambiguity agents that provide domain-specific analysis\n",
    "- **Bidirectional Communication**: The orchestrator can ask questions and receive responses from any specialist based on discussion needs\n",
    "- **Human Approval Gate**: When the orchestrator believes sufficient insights have been gathered, it requests human review before proceeding\n",
    "- **Two-Path Decision**: \n",
    "  - **Approve**: Human accepts the analysis quality and proceeds to report generation\n",
    "  - **Feedback**: Human provides guidance to continue/refine the discussion (shown as dotted line back to orchestrator)\n",
    "- **Report Writer**: Synthesizes the approved conversation into a structured final report\n",
    "- **Final Report**: The comprehensive output with executive summary, findings, analysis, and recommendations\n",
    "\n",
    "The key insight is that the orchestrator dynamically manages the conversation flow rather than following a predetermined sequence, while the human approval step ensures quality control before finalizing results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c5eba",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    User[User] --> Orch[Orchestrator<br/>+ Message History]\n",
    "    \n",
    "    Orch <-->|ask/respond| Critique[Critique<br/>Agent]\n",
    "    Orch <-->|ask/respond| Gap[Gap<br/>Agent]\n",
    "    Orch <-->|ask/respond| Ambig[Ambiguity<br/>Agent]\n",
    "    \n",
    "    Orch ==>|ready for handoff| Human{Human<br/>Approval}\n",
    "    Human -->|approve| Writer[Report<br/>Writer]\n",
    "    Human -.->|feedback| Orch\n",
    "    Writer --> Report[Final<br/>Report]\n",
    "    \n",
    "    style Orch fill:#e1f5ff,stroke:#0066cc,stroke-width:3px\n",
    "    style Human fill:#fff3cd,stroke:#ff9800,stroke-width:3px\n",
    "    style Writer fill:#e8f5e9,stroke:#4caf50,stroke-width:2px\n",
    "    style Report fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9ff08",
   "metadata": {},
   "source": [
    "## Agent Definitions\n",
    "\n",
    "### Orchestrator Agent\n",
    "\n",
    "The orchestrator is the \"swarm manager\" with several key responsibilities:\n",
    "\n",
    "1. **Tool-based coordination**: Uses three tool functions (`ask_critique_agent`, `ask_gap_analysis_agent`, `ask_ambiguity_agent`) to engage specialists\n",
    "2. **Dynamic decision-making**: Chooses which specialist to call based on conversation state, not a fixed pattern\n",
    "3. **Dialogue facilitation**: System prompt instructs it to create connections between agents' observations\n",
    "4. **Handoff decision**: Determines when sufficient analysis has occurred and triggers report generation\n",
    "\n",
    "The orchestrator's output type is set to `handoff_to_report_writer`, making handoff the natural conclusion of its execution.\n",
    "\n",
    "### Specialist Agents\n",
    "\n",
    "Each specialist agent has a focused role with a system prompt designed to encourage collaborative dialogue:\n",
    "\n",
    "**Critique Agent**: Analyzes document writing quality - clarity, structure, tone, and accessibility. Prompted to reference other agents' observations when responding.\n",
    "\n",
    "**Gap Analysis Agent**: Identifies missing information and coverage gaps. Encouraged to connect gaps to issues raised by other agents.\n",
    "\n",
    "**Ambiguity Agent**: Detects unclear or vague language. Prompted to explain how ambiguities might cause problems identified by other specialists.\n",
    "\n",
    "All three specialist agents share:\n",
    "- The same model (Claude Haiku) for consistency\n",
    "- Access to shared message history for context\n",
    "- System prompts emphasizing dialogue and cross-referencing\n",
    "- Tools to query the source policy document\n",
    "\n",
    "### Tool Functions\n",
    "\n",
    "Each tool function (`ask_*_agent`) handles the interaction with a specialist:\n",
    "\n",
    "1. **Logs the question** being asked (for visibility during execution)\n",
    "2. **Calls the specialist agent** with the source policy and question, passing shared message history\n",
    "3. **Records the interaction** in the shared message history as a request/response pair\n",
    "4. **Returns formatted response** to the orchestrator\n",
    "\n",
    "This pattern ensures the conversation is properly tracked and available to all agents.\n",
    "\n",
    "### Output Models\n",
    "\n",
    "**FinalReport**: A Pydantic model defining the structure of the final analysis with fields for executive summary, key findings, detailed analysis, and recommendations.\n",
    "\n",
    "**ReportRejected**: A signal model indicating human feedback was provided instead of approval, prompting continued discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d78c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "# Final report model\n",
    "class FinalReport(BaseModel):\n",
    "    \"\"\"Final comprehensive report\"\"\"\n",
    "\n",
    "    executive_summary: str\n",
    "    key_findings: str\n",
    "    detailed_analysis: str\n",
    "    recommendations: str\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"FinalReport(\\n\"\n",
    "            f\"  Executive Summary: {self.executive_summary}\\n\"\n",
    "            f\"  Key Findings: {self.key_findings}\\n\"\n",
    "            f\"  Detailed Analysis: {self.detailed_analysis}\\n\"\n",
    "            f\"  Recommendations: {self.recommendations}\\n\"\n",
    "            f\")\"\n",
    "        )\n",
    "\n",
    "\n",
    "class ReportRejected(BaseModel):\n",
    "    \"\"\"Indicates that the report was rejected and discussion should continue\"\"\"\n",
    "\n",
    "\n",
    "instrumentation_settings = pydantic_ai.InstrumentationSettings(\n",
    "    include_binary_content=False,\n",
    "    include_content=True,\n",
    ")\n",
    "\n",
    "critique_agent = pydantic_ai.Agent(\n",
    "    model=claude_haiku,\n",
    "    model_settings=bedrock_model_settings,\n",
    "    deps_type=AgentDependencies,\n",
    "    output_type=str,\n",
    "    system_prompt=\"\"\"You are a critique specialist that analyzes how farming policy documents are written.\n",
    "Focus on clarity, structure, tone, accessibility, and communication effectiveness.\n",
    "\n",
    "IMPORTANT: You are part of an active group discussion. Review ALL previous messages carefully:\n",
    "\n",
    "When responding to questions:\n",
    "- ALWAYS acknowledge and reference specific points made by other agents\n",
    "- Build directly on their findings with your unique perspective\n",
    "- Challenge or validate their observations explicitly\n",
    "- Ask follow-up questions when you need clarity from them\n",
    "- Point out connections between your analysis and theirs\n",
    "\n",
    "Examples:\n",
    "- \"I agree with Gap Analysis Agent's point about X. The missing content creates structural issues because...\"\n",
    "- \"Ambiguity Agent, you mentioned Y was unclear. I've noticed this also affects the document flow in section Z...\"\n",
    "- \"Building on what Gap Analysis found, the lack of X makes the tone inconsistent when...\"\n",
    "\n",
    "Be conversational and engaged. This is a dialogue, not a report.\n",
    "\n",
    "Use all the tools at your disposal to reference the target policy document as needed.\"\"\",\n",
    "    instrument=instrumentation_settings,\n",
    ")\n",
    "\n",
    "gap_analysis_agent = pydantic_ai.Agent(\n",
    "    model=claude_haiku,\n",
    "    model_settings=bedrock_model_settings,\n",
    "    deps_type=AgentDependencies,\n",
    "    output_type=str,\n",
    "    system_prompt=\"\"\"You are a gap analysis specialist for farming policies.\n",
    "Identify missing information, overlooked areas, and gaps in coverage.\n",
    "Consider what's not addressed that should be for comprehensive policy.\n",
    "\n",
    "IMPORTANT: You are part of an active group discussion. Review ALL previous messages carefully:\n",
    "\n",
    "When responding to questions:\n",
    "- ALWAYS reference specific observations from other agents\n",
    "- Explain how gaps you find relate to issues they've raised\n",
    "- Directly address their concerns with evidence from the policy\n",
    "- Ask them for clarification or additional perspective\n",
    "- Build collaborative understanding\n",
    "\n",
    "Examples:\n",
    "- \"Critique Agent, your concern about structure is spot-on. The document is missing...\"\n",
    "- \"Responding to Ambiguity Agent's point about vague language - I see the same pattern in missing definitions...\"\n",
    "- \"This gap I found might explain why Critique Agent observed X. Can we explore that connection?\"\n",
    "\n",
    "Engage actively with the conversation. Reference others' names and specific points.\n",
    "\n",
    "Use all the tools at your disposal to reference the target policy document as needed.\"\"\",\n",
    "    instrument=instrumentation_settings,\n",
    ")\n",
    "\n",
    "ambiguity_agent = pydantic_ai.Agent(\n",
    "    model=claude_haiku,\n",
    "    model_settings=bedrock_model_settings,\n",
    "    deps_type=AgentDependencies,\n",
    "    output_type=str,\n",
    "    system_prompt=\"\"\"You are an ambiguity detection specialist.\n",
    "Identify unclear language, vague requirements, and areas open to interpretation.\n",
    "Highlight potential confusion points and suggest clarifications.\n",
    "\n",
    "IMPORTANT: You are part of an active group discussion. Review ALL previous messages carefully:\n",
    "\n",
    "When responding to questions:\n",
    "- ALWAYS connect your findings to what other agents have said\n",
    "- Explain how vague language might be causing the issues they identified\n",
    "- Directly address their specific concerns\n",
    "- Ask for their input on whether ambiguities explain their observations\n",
    "- Create dialogue by referencing their names and points\n",
    "\n",
    "Examples:\n",
    "- \"Gap Analysis Agent found X is missing. I wonder if that's because the language around it is so vague...\"\n",
    "- \"Critique Agent, the structure issue you noted - could it stem from this ambiguous terminology?\"\n",
    "- \"I agree with both of you. The vagueness I'm seeing in section Y connects to both the gaps and structure problems...\"\n",
    "\n",
    "Be conversational and collaborative. Make this a real discussion.\n",
    "\n",
    "Use all the tools at your disposal to reference the target policy document as needed.\"\"\",\n",
    "    instrument=instrumentation_settings,\n",
    ")\n",
    "\n",
    "report_writer_agent = pydantic_ai.Agent(\n",
    "    model=claude_sonnet,\n",
    "    model_settings=bedrock_model_settings,\n",
    "    deps_type=AgentDependencies,\n",
    "    output_type=str,\n",
    "    system_prompt=\"\"\"You are a report writing specialist.\n",
    "Synthesize all the group discussion into a comprehensive, well-structured report.\n",
    "Review the entire conversation history and create a cohesive analysis.\n",
    "\n",
    "Use all the tools at your disposal to reference the target policy document as needed.\"\"\",\n",
    "    instrument=instrumentation_settings,\n",
    ")\n",
    "\n",
    "\n",
    "# Output function for handoff to report writer with human approval\n",
    "async def handoff_to_report_writer(\n",
    "    ctx: pydantic_ai.RunContext[AgentDependencies],\n",
    ") -> FinalReport | ReportRejected:\n",
    "    \"\"\"Hand off to the report writer to synthesize all discussion into a final report.\n",
    "    Use this when the group discussion has produced sufficient insights.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ðŸŽ¯ ORCHESTRATOR READY FOR REVIEW\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nThe orchestrator has completed the group discussion.\\n\")\n",
    "    print(f\"Total messages in conversation: {len(ctx.deps.messages)}\\n\")\n",
    "\n",
    "    # Show summary of conversation\n",
    "    print(\"ðŸ“‹ CONVERSATION SUMMARY:\")\n",
    "    print(ctx.deps.messages[-10:])  # Show last 10 messages for brevity\n",
    "\n",
    "    decision = input(\n",
    "        \"Do you approve generating the final report? (yes/approve to proceed, any other input to provide feedback): \"\n",
    "    ).strip()\n",
    "\n",
    "    if decision.lower() in [\"yes\", \"approve\", \"y\"]:\n",
    "        print(\"\\nâœ… Approved! Generating final report...\\n\")\n",
    "        print(\"ðŸ”„ Handing off to Report Writer Agent...\")\n",
    "\n",
    "        # Pass the conversation history to the report writer\n",
    "        response = await report_writer_agent.run(\n",
    "            user_prompt=\"Review the entire conversation history and create a comprehensive report synthesizing all findings.\",\n",
    "            deps=ctx.deps,\n",
    "            output_type=FinalReport,\n",
    "            message_history=ctx.deps.messages,\n",
    "        )\n",
    "\n",
    "        print(\"âœ… Report Writer completed\\n\")\n",
    "        return response.output\n",
    "    feedback = input(\"Please provide your feedback for the agents: \").strip()\n",
    "    print(\"\\nðŸ”„ Feedback received. Continuing discussion...\\n\")\n",
    "    # Return feedback as string (not a FinalReport)\n",
    "    # This will be caught in the execution loop\n",
    "    return f\"[User]: Output Rejected - {feedback}\"\n",
    "\n",
    "\n",
    "# Orchestrator agent with tools to coordinate the group chat\n",
    "orchestrator_agent = pydantic_ai.Agent(\n",
    "    model=claude_haiku,\n",
    "    model_settings=bedrock_model_settings,\n",
    "    deps_type=AgentDependencies,\n",
    "    output_type=handoff_to_report_writer,\n",
    "    instrument=instrumentation_settings,\n",
    "    system_prompt=\"\"\"You are an orchestrator coordinating a group discussion about farming policy analysis.\n",
    "\n",
    "You have three specialist agents available:\n",
    "- Critique Agent: analyzes how the document is written\n",
    "- Gap Analysis Agent: identifies missing information and coverage gaps\n",
    "- Ambiguity Agent: finds unclear or vague language\n",
    "\n",
    "Your role is to FACILITATE dialogue between the agents:\n",
    "\n",
    "1. Start by getting perspectives from the agents who are most relevant to the analysis\n",
    "2. When an agent makes an observation, consider whether other agents should respond:\n",
    "   - Ask agents to build on each other's findings\n",
    "   - Direct follow-up questions when connections emerge\n",
    "   - Example: \"Ambiguity Agent, Critique Agent mentioned unclear structure. Does vague language contribute to this?\"\n",
    "3. Create natural back-and-forth by having agents respond to each other's specific points\n",
    "4. Push for concrete examples when agents make general observations\n",
    "\n",
    "IMPORTANT PRINCIPLES:\n",
    "- ALWAYS include the agent's name in your question\n",
    "- Reference specific points from other agents when creating dialogue\n",
    "- Make agents talk TO each other, not just to you\n",
    "- If an agent doesn't reference others, consider asking them to engage with prior observations\n",
    "\n",
    "When you feel the discussion has produced sufficient insights to answer the analysis comprehensively, hand off to the report writer. Don't force unnecessary rounds - quality over quantity.\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "@orchestrator_agent.tool\n",
    "async def ask_critique_agent(\n",
    "    ctx: pydantic_ai.RunContext[AgentDependencies], question: str\n",
    ") -> str:\n",
    "    \"\"\"Ask the critique agent to analyze how the document is written.\"\"\"\n",
    "\n",
    "    print(f\"\\nðŸ“ Critique Agent: {question[:100]}...\")\n",
    "\n",
    "    response = await critique_agent.run(\n",
    "        user_prompt=[ctx.deps.source_policy, question],\n",
    "        deps=ctx.deps,\n",
    "        message_history=ctx.deps.messages,\n",
    "    )\n",
    "\n",
    "    # Add to message history as a request/response pair\n",
    "    ctx.deps.messages.append(\n",
    "        ModelRequest(\n",
    "            parts=[UserPromptPart(content=f\"[Critique Agent] {question}\")],\n",
    "            timestamp=None,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    formatted = f\"[Critique Agent] {response.output}\"\n",
    "\n",
    "    ctx.deps.messages.append(\n",
    "        ModelResponse(parts=[TextPart(content=formatted)], timestamp=None)\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Response received ({len(response.output)} chars)\\n\")\n",
    "    return formatted\n",
    "\n",
    "\n",
    "@orchestrator_agent.tool\n",
    "async def ask_gap_analysis_agent(\n",
    "    ctx: pydantic_ai.RunContext[AgentDependencies], question: str\n",
    ") -> str:\n",
    "    \"\"\"Ask the gap analysis agent to identify missing information or coverage gaps.\"\"\"\n",
    "\n",
    "    print(f\"\\nðŸ” Gap Analysis Agent: {question[:100]}...\")\n",
    "\n",
    "    response = await gap_analysis_agent.run(\n",
    "        user_prompt=[ctx.deps.source_policy, question],\n",
    "        deps=ctx.deps,\n",
    "        message_history=ctx.deps.messages,\n",
    "    )\n",
    "\n",
    "    # Add to message history as a request/response pair\n",
    "    ctx.deps.messages.append(\n",
    "        ModelRequest(\n",
    "            parts=[UserPromptPart(content=f\"[Gap Analysis Agent] {question}\")],\n",
    "            timestamp=None,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    formatted = f\"[Gap Analysis Agent] {response.output}\"\n",
    "\n",
    "    ctx.deps.messages.append(\n",
    "        ModelResponse(parts=[TextPart(content=formatted)], timestamp=None)\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Response received ({len(response.output)} chars)\\n\")\n",
    "    return formatted\n",
    "\n",
    "\n",
    "@orchestrator_agent.tool\n",
    "async def ask_ambiguity_agent(\n",
    "    ctx: pydantic_ai.RunContext[AgentDependencies], question: str\n",
    ") -> str:\n",
    "    \"\"\"Ask the ambiguity agent to identify unclear or vague language.\"\"\"\n",
    "\n",
    "    print(f\"\\nâ“ Ambiguity Agent: {question[:100]}...\")\n",
    "\n",
    "    response = await ambiguity_agent.run(\n",
    "        user_prompt=[ctx.deps.source_policy, question],\n",
    "        deps=ctx.deps,\n",
    "        message_history=ctx.deps.messages,\n",
    "    )\n",
    "\n",
    "    # Add to message history as a request/response pair\n",
    "    ctx.deps.messages.append(\n",
    "        ModelRequest(\n",
    "            parts=[UserPromptPart(content=f\"[Ambiguity Agent] {question}\")],\n",
    "            timestamp=None,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    formatted = f\"[Ambiguity Agent] {response.output}\"\n",
    "\n",
    "    ctx.deps.messages.append(\n",
    "        ModelResponse(parts=[TextPart(content=formatted)], timestamp=None)\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Response received ({len(response.output)} chars)\\n\")\n",
    "    return formatted\n",
    "\n",
    "@orchestrator_agent.system_prompt\n",
    "def system_prompt() -> str:\n",
    "    return \"\"\"You are an orchestrator coordinating a group discussion about farming policy analysis.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc467871",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "### Policy Selection\n",
    "\n",
    "Select which farming policy document to analyze by setting the `policy_name` variable. The policy content is loaded from markdown files scraped by the `farming-funding.ipynb` notebook.\n",
    "\n",
    "The `AgentDependencies` object is initialized with the policy text and an empty message history.\n",
    "\n",
    "### Orchestration Loop\n",
    "\n",
    "The orchestrator is executed with a prompt to facilitate comprehensive analysis. Key execution parameters:\n",
    "\n",
    "- **UsageLimits**: Each orchestrator run gets 25 tool calls to prevent runaway execution. Note that if the human provides feedback and the loop continues, the orchestrator gets a fresh 25 tool calls for the next iteration.\n",
    "- **Message history**: Maintains conversation continuity across all runs, even when usage limits reset\n",
    "\n",
    "**The handoff function** (`handoff_to_report_writer`) provides human-in-the-loop control:\n",
    "1. Displays conversation summary when orchestrator is ready for handoff\n",
    "2. Prompts human to approve or provide feedback\n",
    "3. On approval: Calls report writer agent to synthesize findings\n",
    "4. On rejection: Returns feedback string to continue discussion\n",
    "\n",
    "**The continuation loop** checks if the output is a `FinalReport`:\n",
    "- If not (feedback was provided), it continues the orchestrator run with the feedback and fresh usage limits\n",
    "- If yes, the loop exits and the final report is available\n",
    "\n",
    "This pattern ensures humans can guide the analysis quality and depth before finalizing results, with the ability to run multiple rounds of analysis if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b77fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "policy_name = \"AGF1\"\n",
    "policy_path = pathlib.Path(f\"../scrapers/outputs/{policy_name.upper()}.md\")\n",
    "\n",
    "# Initialize dependencies\n",
    "deps = AgentDependencies(source_policy=policy_path.read_text())\n",
    "\n",
    "operation_usage: list[pydantic_ai.RunUsage] = []\n",
    "\n",
    "# Run the orchestrator to coordinate the group chat\n",
    "orchestrator_response = await orchestrator_agent.run(\n",
    "    user_prompt=[\n",
    "        \"\"\"Facilitate a group discussion to comprehensively analyze this farming policy document.\n",
    "    Let the agents respond to each other's findings and build on the discussion.\n",
    "    When you've gathered sufficient insights (aim for thorough coverage), generate the final report.\"\"\",\n",
    "        deps.source_policy,\n",
    "    ],\n",
    "    deps=deps,\n",
    "    usage_limits=pydantic_ai.UsageLimits(tool_calls_limit=25),\n",
    ")\n",
    "\n",
    "operation_usage.append(orchestrator_response.usage)\n",
    "\n",
    "while not isinstance(orchestrator_response.output, FinalReport):\n",
    "    feedback = orchestrator_response.output\n",
    "    print(f\"\\nðŸ”„ Continuing discussion based on feedback:\\n{feedback}\\n\")\n",
    "\n",
    "    # Continue the orchestrator run with the feedback\n",
    "    orchestrator_response = await orchestrator_agent.run(\n",
    "        user_prompt=[\n",
    "            deps.source_policy,\n",
    "            f\"{feedback}\\n\\nContinue the group discussion to address the feedback and work towards a final report.\",\n",
    "        ],\n",
    "        deps=deps,\n",
    "        usage_limits=pydantic_ai.UsageLimits(tool_calls_limit=25),\n",
    "    )\n",
    "\n",
    "    operation_usage.append(orchestrator_response.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6d7a9",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### View Final Report\n",
    "\n",
    "Display the final synthesized report if one was generated. The report follows the `FinalReport` model structure with executive summary, key findings, detailed analysis, and recommendations.\n",
    "\n",
    "If the execution was stopped before handoff approval, no report will be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "orchestrator_response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e444b",
   "metadata": {},
   "source": [
    "### View Complete Trace\n",
    "\n",
    "Optional diagnostic output showing the full message history and tool calls in JSON format. Useful for:\n",
    "- Understanding the orchestrator's decision-making process\n",
    "- Debugging unexpected behavior\n",
    "- Analyzing conversation flow patterns\n",
    "- Evaluating agent interaction quality\n",
    "\n",
    "This provides the raw execution trace that can be correlated with OpenTelemetry spans if tracing is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8309f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: View the complete message history and tool calls\n",
    "print(\"=== COMPLETE MESSAGE TRACE ===\\n\")\n",
    "pprint.pprint(str(orchestrator_response.all_messages_json()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-spike-agent-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
